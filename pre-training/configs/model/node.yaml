# @package _global_

order: 2
num_vecs: 512
normalized_laplacian: yes
normalize_eigenvecs: no
max_degree: 5000
num_phi_layers: 2
num_rho_layers: 2
phi_dim: 16
embed_dim: 512
eigen_dim: 512
inner_dim: 96
degree_dim: 16
num_layers: 3
num_heads: 2
attention_dropout: 0.1
dropout: 0.1
linear_attention: yes
bias: no
stochastic_depth: no
token_ln: yes
pe_type: LPE
spe_lower_rank: 384
